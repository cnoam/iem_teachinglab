{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "feb3475a-f093-4506-930f-d5222c78cafb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Demo: reading from our Kafka server\n",
    "This notebook reads from topic 'noam' which contains a few strings.\n",
    "If it works it means that all the parts are working together. It does NOT means there will not be performance problems down the road."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e730311-f4b6-4889-9cd4-5419a548a003",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.types import *\n",
    "import os,time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "391c00b4-fa08-4076-9b5e-9505b998fbeb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 905 µs, sys: 0 ns, total: 905 µs\nWall time: 12.4 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "# The config packages must match the specific Spark version you run!\n",
    "spark = SparkSession.builder.appName('streaming')\\\n",
    "    .config(\"spark.kryoserializer.buffer.max\", \"512m\")\\\n",
    "    .config('spark.jars.packages', 'org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.0')\\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "464260ca-f067-4ca0-b344-ec3825c6e77e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "kafka_server = \"kafka96224.eastus.cloudapp.azure.com:29092\" \n",
    "topic = \"noam\"             # the topic name where the data is stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cdd2b99c-a628-4624-a5c9-f59a81cbf98d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SCHEMA = \"name STRING\"\n",
    "static_df = spark.read\\\n",
    "                  .format(\"kafka\")\\\n",
    "                  .option(\"kafka.bootstrap.servers\", kafka_server)\\\n",
    "                  .option(\"subscribe\", topic)\\\n",
    "                  .option(\"startingOffsets\", \"earliest\")\\\n",
    "                  .option(\"failOnDataLoss\",False)\\\n",
    "                  .load()\n",
    "retail_data = static_df.select(f.from_csv(f.decode(\"value\", \"US-ASCII\"), schema=SCHEMA).alias(\"value\")).select(\"value.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1144b0f-405f-4890-98ec-31d46ed3a1c9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 records in frame\n+----------------+\n|            name|\n+----------------+\n|             one|\n|             two|\n|           three|\n|four four1 four2|\n|             six|\n+----------------+\n\nCPU times: user 8.98 ms, sys: 0 ns, total: 8.98 ms\nWall time: 7.57 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "print(\"%d records in frame\" % retail_data.count())\n",
    "retail_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4bc48058-cee7-494d-b49d-1b2eaf24edd9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+-----+---------+------+--------------------+-------------+\n| key|               value|topic|partition|offset|           timestamp|timestampType|\n+----+--------------------+-----+---------+------+--------------------+-------------+\n|null|          [6F 6E 65]| noam|        0|     0|2023-06-16 14:15:...|            0|\n|null|          [74 77 6F]| noam|        0|     1|2023-06-16 14:15:...|            0|\n|null|    [74 68 72 65 65]| noam|        0|     2|2023-06-16 14:15:...|            0|\n|null|[66 6F 75 72 20 6...| noam|        0|     3|2023-06-16 14:15:...|            0|\n|null|          [73 69 78]| noam|        0|     4|2023-06-16 14:15:...|            0|\n+----+--------------------+-----+---------+------+--------------------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "static_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78b21649-9ee5-43dd-b932-b56e6feb735c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "consuming from kafka",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
