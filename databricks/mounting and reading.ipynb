{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "01807ea7-1080-4665-872a-8b4a8bcab008",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Mount an Azure Blob Container as a directory in the linux file system (in the Worker node)\n",
    "This has to be done once - the mounting is remembered between cluster restarts.\n",
    "\n",
    "The mount point is in a distributed file system called DBUTILS -- a product of Databricks.\n",
    "\n",
    "See the documentation at  https://docs.databricks.com/files/index.html  to understand the difference between this mount point and using the local storage of a cluster\n",
    "\n",
    "This file is part of https://github.com/cnoam/iem_teachinglab.git in 'databricks' folder\n",
    "\n",
    "<br>\n",
    "\n",
    "**NOTE** In 2024, DBR hardened the security, so you need to whitelist the method. \n",
    "\n",
    "This can be done by connecting to a cluster that is in **\"Access Mode: Dedicated\"**(formerly single user mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e8f8b87-a2ce-479e-af65-8d702b522fe3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "storage_account = \"coursedata2024\"  \n",
    "container = \"fwm-stb-data\"  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "195f0690-f048-482f-9eff-feed4cb225f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# If you want to make sure the mount point is unmounted, uncomment this\n",
    "# WARNING WARNING WARNING: IT LOOKS LIKE DBR REMEMEBERS LOST MOUNTS. VERY WEIRD.\n",
    "\"\"\"Key takeaway: You do not have to mount a container to use it in Databricks; mounting just creates a “friendly” DBFS path. If you have credentials set at the cluster/session level, you can directly access wasbs://... or abfss://... paths (or the /mnt/... DBFS equivalent) without an explicit mount. That’s why dbutils.fs.ls(\"/mnt/...\") can still succeed even though dbutils.fs.mounts() does not show it.\n",
    "\"\"\"\n",
    "dbutils.fs.unmount(f\"/mnt/{storage_account}/{container}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c581d14c-4d24-4ae8-9a72-54da711cc4ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# This code uses SAS token authentication to access the data\n",
    "secret = \"sp=rl&st=2025-03-16T08:29:06Z&se=2025-09-01T15:29:06Z&spr=https&sv=2022-11-02&sr=c&sig=kv1VUVNTHUxwMUUUpw7z3duDFfn6CHEIvbgeGcRygkM%3D\"\n",
    "mount_point   = \"/mnt/{storage_account}/{container}\".format(storage_account = storage_account, container = container)  \n",
    "\n",
    "if mount_point not in [m.mountPoint for m in dbutils.fs.mounts()]:  \n",
    "                dbutils.fs.mount(  \n",
    "                   source        = \"wasbs://{container}@{storage_account}.blob.core.windows.net/\".format(container = container, storage_account = storage_account),  \n",
    "                   mount_point   = mount_point,  \n",
    "                   extra_configs = {f\"fs.azure.sas.{container}.{storage_account}.blob.core.windows.net\" : secret}  \n",
    "                 )\n",
    "                print(\"Mounted ok\")\n",
    "else:\n",
    "    print(f\"{mount_point} is already mounted\")\n",
    "  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff8295fb-1a52-49b7-899f-afae88c50f02",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# list the mount points\n",
    "dbutils.fs.mounts()\n",
    "#dbutils.fs.unmount(f'/mnt/{storage_account}/{container}/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5bf60fca-1adf-4858-ab7a-32b2bccf4371",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.ls(f\"/mnt/{storage_account}/{container}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf4e9c43-c02a-407e-992e-4c8b5b0833c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Modify this line to try to read an existing file. It should fail\n",
    "fname = dbutils.fs.ls(f\"/mnt/{storage_account}/{container}/demographic/\") \n",
    "if fname[0].name != 'SintecMedia.rpt_demodata.date_2015-12-31.2016-01-01.pd.gz':\n",
    "    raise Exception(\"File name is not expected!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "227320ae-81b3-4b2c-b696-15f2121efad0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "opening the file with open() should fail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12bf4cd2-149f-4ce7-9f8a-730fdd354644",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "try: open(f\"dbfs:/mnt/{storage_account}/{container}/demographic/SintecMedia.rpt_demodata.date_2015-12-31.2016-01-01.pd.gz\", \"r\")\n",
    "except Exception as ex: print(\"calling open() will fail!\\n\" + str(ex))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e632e841-3ab1-4e8a-9385-6a216a65efb8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Read XML file\n",
    "\n",
    "An external library is needed to be installed in the cluster.\n",
    "\n",
    "Follow the instructions in https://learn.microsoft.com/en-us/azure/databricks/libraries/cluster-libraries. <br>\n",
    "The library name is `com.databricks:spark-xml_2.12:0.16.0`\n",
    "\n",
    "and can be downloaded from `https://repo1.maven.org/maven2/com/databricks/spark-xml_2.12/0.16.0/spark-xml_2.12-0.16.0.jar`\n",
    "\n",
    "or installed directly using the Maven coordinates\n",
    "\n",
    "2025-05-28 update:  running this notebook on cluster \"15.4 LTS (includes Apache Spark 3.5.0, Scala 2.12)\", without any libraries installed, the run succeeded. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ec9c17a-0578-42d8-83a3-3e010cccd259",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fname = f\"dbfs:/mnt/{storage_account}/fwm-stb-data/refxml/SintecMedia.rpt_refxml.date_2015-01-01.2016-11-21.xml.gz\"\n",
    "df = spark.read.format(\"xml\").option(\"compression\",\"gzip\").option(\"rowTag\", \"mapping\").load(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2072feb-4cd6-41a2-a4d8-f9fd6af65cee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "516329d0-cd71-46b1-b1ea-7cac53dcf87a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Try to write to the mount point. We should fail with Permission Denied\n",
    "empty_df = spark.createDataFrame([], df.schema)\n",
    "try:\n",
    "    empty_df.write.mode(\"overwrite\").parquet(f\"dbfs:/mnt/{storage_account}/fwm-stb-data/refxml/dummy\")\n",
    "except Exception as ex:\n",
    "    #print(\"Writing to mount point should fail!\\n\" + str(ex))\n",
    "    if \"This request is not authorized to perform this operation using this permission\" not in str(ex):\n",
    "        raise Exception(\"❌ Writing to mount point should fail, but it SUCCEDED!\\n\" + str(ex))\n",
    "    else:\n",
    "        print(\"✅ Writing to mount point failed as it should be!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8debe556-5b68-40de-b32f-559728beb6b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ed6b4a3-3b0a-4c47-a0d6-04e1bc26f3a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import window, column, desc, col\n",
    "df.filter(col('_system-type') != 'H').show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d30250b-d5c0-43c0-b218-05fedf8c66b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# how many records generated from each of the system types?\n",
    "df.groupBy(\"_system-type\").count().show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 2411910607236802,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "mounting and reading",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
